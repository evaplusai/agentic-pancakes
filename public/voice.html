<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Voice Test</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #1a1a2e;
            color: white;
            min-height: 100vh;
            padding: 20px;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
            font-size: 24px;
        }
        .container {
            max-width: 500px;
            margin: 0 auto;
        }
        .mic-btn {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-size: 48px;
            cursor: pointer;
            display: block;
            margin: 30px auto;
            transition: all 0.3s;
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
        }
        .mic-btn:active {
            transform: scale(0.95);
        }
        .mic-btn.recording {
            background: linear-gradient(135deg, #f5576c 0%, #f093fb 100%);
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0%, 100% { box-shadow: 0 0 0 0 rgba(245, 87, 108, 0.7); }
            50% { box-shadow: 0 0 0 20px rgba(245, 87, 108, 0); }
        }
        .status {
            text-align: center;
            font-size: 18px;
            margin: 20px 0;
            color: #888;
        }
        .status.recording {
            color: #f5576c;
        }
        .transcript-box {
            background: rgba(255,255,255,0.1);
            border-radius: 12px;
            padding: 20px;
            min-height: 150px;
            margin: 20px 0;
        }
        .transcript-box h3 {
            font-size: 14px;
            color: #888;
            margin-bottom: 10px;
        }
        .transcript {
            font-size: 20px;
            line-height: 1.5;
            word-wrap: break-word;
        }
        .transcript.partial {
            color: #888;
            font-style: italic;
        }
        .log {
            background: rgba(0,0,0,0.3);
            border-radius: 8px;
            padding: 15px;
            margin-top: 20px;
            font-family: monospace;
            font-size: 12px;
            max-height: 200px;
            overflow-y: auto;
        }
        .log-entry {
            margin: 5px 0;
            padding: 5px;
            border-bottom: 1px solid rgba(255,255,255,0.1);
        }
        .log-entry.error {
            color: #f5576c;
        }
        .log-entry.success {
            color: #4ade80;
        }
        .info {
            background: rgba(102, 126, 234, 0.2);
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Voice Test</h1>

        <div class="info">
            Testing Modal Whisper endpoint on iOS.<br>
            Tap mic, speak, tap again to stop.
        </div>

        <button class="mic-btn" id="micBtn">ðŸŽ¤</button>

        <div class="status" id="status">Tap mic to start</div>

        <div class="transcript-box">
            <h3>Live Transcript:</h3>
            <div class="transcript" id="liveTranscript">-</div>
        </div>

        <div class="transcript-box">
            <h3>Final Result:</h3>
            <div class="transcript" id="finalTranscript">-</div>
        </div>

        <div class="log" id="log"></div>
    </div>

    <script>
        const micBtn = document.getElementById('micBtn');
        const status = document.getElementById('status');
        const liveTranscript = document.getElementById('liveTranscript');
        const finalTranscript = document.getElementById('finalTranscript');
        const logDiv = document.getElementById('log');

        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let chunkInterval = null;
        let chunkCounter = 0;
        let streamingText = '';
        let audioContext = null;
        let analyser = null;
        let hasDetectedSpeech = false;

        function log(msg, type = 'info') {
            const entry = document.createElement('div');
            entry.className = `log-entry ${type}`;
            entry.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
            logDiv.insertBefore(entry, logDiv.firstChild);
            console.log(msg);
        }

        micBtn.addEventListener('click', async () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });

        async function startRecording() {
            try {
                log('Requesting microphone access...');

                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 16000
                    }
                });

                log('Microphone access granted', 'success');

                const mimeType = MediaRecorder.isTypeSupported('audio/webm')
                    ? 'audio/webm'
                    : 'audio/mp4';
                log(`Using mime type: ${mimeType}`);

                // Set up Voice Activity Detection (VAD)
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                const source = audioContext.createMediaStreamSource(stream);
                source.connect(analyser);
                analyser.fftSize = 512;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                hasDetectedSpeech = false;

                mediaRecorder = new MediaRecorder(stream, { mimeType });
                audioChunks = [];
                chunkCounter = 0;
                streamingText = '';
                liveTranscript.textContent = 'Listening... speak now';
                liveTranscript.className = 'transcript partial';
                finalTranscript.textContent = '-';

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    log('Recording stopped');
                    stream.getTracks().forEach(track => track.stop());

                    if (audioContext) {
                        audioContext.close();
                        audioContext = null;
                    }

                    if (chunkInterval) {
                        clearInterval(chunkInterval);
                        chunkInterval = null;
                    }

                    // Final transcription with full audio
                    if (audioChunks.length > 0) {
                        const fullBlob = new Blob(audioChunks, { type: mimeType });
                        log(`Sending full audio for final transcription: ${fullBlob.size} bytes`);
                        await transcribeFull(fullBlob);
                    }
                };

                mediaRecorder.start(100); // Get data every 100ms for smoother VAD
                isRecording = true;

                micBtn.classList.add('recording');
                status.textContent = 'Listening... (auto-stops on pause)';
                status.classList.add('recording');

                log('Recording started with VAD', 'success');

                // Voice Activity Detection loop
                let silenceStart = null;
                const SILENCE_THRESHOLD = 15;
                const SILENCE_DURATION = 1500; // 1.5 seconds of silence to stop
                const SPEECH_THRESHOLD = 25;

                function checkAudioLevel() {
                    if (!isRecording) return;

                    analyser.getByteFrequencyData(dataArray);
                    const average = dataArray.reduce((a, b) => a + b, 0) / bufferLength;

                    // Detect speech
                    if (average > SPEECH_THRESHOLD) {
                        if (!hasDetectedSpeech) {
                            log(`Speech detected! Level: ${average.toFixed(1)}`, 'success');
                            liveTranscript.textContent = 'Hearing you...';
                        }
                        hasDetectedSpeech = true;
                        silenceStart = null;
                    }

                    // After detecting speech, check for silence
                    if (hasDetectedSpeech && average < SILENCE_THRESHOLD) {
                        if (!silenceStart) {
                            silenceStart = Date.now();
                            log(`Silence started, waiting ${SILENCE_DURATION}ms...`);
                        } else if (Date.now() - silenceStart > SILENCE_DURATION) {
                            log('Silence detected - auto-stopping', 'success');
                            stopRecording();
                            return;
                        }
                    } else if (average >= SILENCE_THRESHOLD) {
                        silenceStart = null;
                    }

                    requestAnimationFrame(checkAudioLevel);
                }
                checkAudioLevel();

                // Send chunks every 2 seconds for live transcription
                chunkInterval = setInterval(async () => {
                    if (audioChunks.length > 0 && hasDetectedSpeech) {
                        const chunkBlob = new Blob([...audioChunks], { type: mimeType });
                        if (chunkBlob.size > 3000) {
                            log(`Sending chunk ${chunkCounter}: ${chunkBlob.size} bytes`);
                            await transcribeChunk(chunkBlob, chunkCounter++);
                        }
                    }
                }, 2000);

                // Fallback: Auto-stop after 15 seconds max
                setTimeout(() => {
                    if (isRecording) {
                        log('Max recording time reached - stopping');
                        stopRecording();
                    }
                }, 15000);

            } catch (error) {
                log(`Error: ${error.message}`, 'error');
                status.textContent = `Error: ${error.message}`;
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                isRecording = false;
                micBtn.classList.remove('recording');
                status.textContent = 'Processing...';
                status.classList.remove('recording');
            }
        }

        async function transcribeChunk(blob, chunkId) {
            try {
                const base64 = await blobToBase64(blob);
                log(`Chunk ${chunkId} base64 size: ${base64.length} chars`);

                const response = await fetch('/api/transcribe-chunk', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio_base64: base64,
                        language: 'en',
                        chunk_id: chunkId
                    })
                });

                const result = await response.json();
                log(`Chunk ${chunkId} response: ${JSON.stringify(result)}`);

                if (result.text && result.text.trim()) {
                    streamingText = result.text.trim();
                    liveTranscript.textContent = `"${streamingText}"`;
                    log(`Live transcript: ${streamingText}`, 'success');
                } else if (result.error) {
                    log(`Chunk error: ${result.error}`, 'error');
                }
            } catch (error) {
                log(`Chunk fetch error: ${error.message}`, 'error');
            }
        }

        async function transcribeFull(blob) {
            try {
                const base64 = await blobToBase64(blob);
                log(`Full audio base64 size: ${base64.length} chars`);

                status.textContent = 'Transcribing full audio...';

                const response = await fetch('/api/transcribe', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        audio_base64: base64,
                        language: 'en'
                    })
                });

                const result = await response.json();
                log(`Full transcription response: ${JSON.stringify(result)}`);

                if (result.text && result.text.trim()) {
                    finalTranscript.textContent = result.text.trim();
                    finalTranscript.className = 'transcript';
                    liveTranscript.className = 'transcript';
                    status.textContent = 'Done! Tap mic to try again';
                    log(`Final transcript: ${result.text}`, 'success');
                } else if (result.error) {
                    finalTranscript.textContent = `Error: ${result.error}`;
                    status.textContent = 'Error - check log';
                    log(`Transcription error: ${result.error}`, 'error');
                } else {
                    finalTranscript.textContent = '(no speech detected)';
                    status.textContent = 'No speech detected';
                    log('No speech detected in audio');
                }
            } catch (error) {
                log(`Full transcription error: ${error.message}`, 'error');
                finalTranscript.textContent = `Error: ${error.message}`;
                status.textContent = 'Error - check log';
            }
        }

        function blobToBase64(blob) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onloadend = () => {
                    const base64 = reader.result.split(',')[1];
                    resolve(base64);
                };
                reader.onerror = reject;
                reader.readAsDataURL(blob);
            });
        }

        log('Voice test page loaded');
        log(`User Agent: ${navigator.userAgent}`);
    </script>
</body>
</html>
